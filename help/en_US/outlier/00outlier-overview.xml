<?xml version="1.0" encoding="UTF-8"?>
<!--
 *
 * Overview SampleSTAT - Outlier-functions
 * Copyright (c) 2019 - Hani A. Ibrahim
 *
-->
<refentry xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:svg="http://www.w3.org/2000/svg" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:db="http://docbook.org/ns/docbook" xmlns:scilab="http://www.scilab.org" xml:lang="en_US" xml:id="outlier_overview">
	<refnamediv>
		<refname>Overview - Outlier Tests</refname>
		<refpurpose>An overview of the SampleSTAT's Outlier Functions</refpurpose>
	</refnamediv>
	<refsection>
		<title>Introduction</title>
		<para>
        Outliers are extreme values that stand out from the other values of a sample. Outliers normally have a considerable influence on the calculation of statistics (see e.g. the leverage effect with linear regression) and should be removed in most cases. You should also note that outliers may result simply from the fact that you assume a distribution which does not fit the real distribution of the data.
    </para>
		<para>
		Typical examples of outliers are errors in measurement, errors in acquisition (human influences...), or (rare) outstanding values. An important question concerning outliers is whether it is legitimate to remove a particular value after it has been recognized as an outlier.  Of course, statistical tests cannot decide, if it is appropriate to remove such values. They can only give you a hint if a significant deviation exists (basically, outlier tests are based on the probabilities to belong to the assumed distribution). 
	</para>
		<para>
	    Tests on outliers in data sets can be used to
        <itemizedlist><listitem><para>Check if methods of measurement are reliable</para></listitem><listitem><para>Check the reliability of data sets</para></listitem></itemizedlist>
	</para>
		<para>
	   Several outlier tests are available, each of them having its own special advantages and drawbacks. Following is a short description of the most commonly used strategies to identify outliers:

           <itemizedlist><listitem><para>Basic rules using the standard deviation and the interquartile range</para></listitem><listitem><para>Dean-Dixon Test</para></listitem><listitem><para>Significance of extreme values (Pearson-Hartley)</para></listitem><listitem><para>Nalimov test</para></listitem><listitem><para> Outlier test according to Walsh (for not normal distributed data, not provided in SampleSTAT)</para></listitem><listitem><para>Grubbs test for outliers (not provided in SampleSTAT - yet)</para></listitem></itemizedlist>

	</para>
		<para>
	   Nonetheless there is one common rule which should be strictly obeyed: do not eliminate outliers successively. To be specific, do not identify an outlier, eliminate it and recalculate the statistics to perform another outlier detection and elimination:
	</para>
		<important>
			<para>
		All outliers must be removed in one single calculation.
	</para>
		</important>
		<important>
			<para>
		The outlier tests of SampleSTAT are applicable for normal distributed data ONLY.
	</para>
		</important>
	</refsection>
	<refsection>
		<title>Test recommendations</title>
		<para>
			<variablelist>
				<varlistentry>
					<term>Dean-Dixon (ST_deandixon):</term>
					<listitem>
						<para> Best for small samples sizes (less than 30). It can just consider one value on each side of the sorted data an outlier which is not a problem for small data sets. </para>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Pearson-Hartley's Significance of Extreme Values (ST_personhardley):</term>
					<listitem>
						<para> For larger sample sizes (more than 30)</para>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Nalimov (ST_nalimov):</term>
					<listitem>
						<para>For small and larger sample sizes. It has a very strict algorithm and is controversially discussed. It was very common in Germany. Use it with care. </para>
					</listitem>
				</varlistentry>
				<varlistentry>
					<term>Basic rules (ST_outlier):</term>
					<listitem>
						<para>Up from medium sample sizes (larger than 10, better more than 25). </para>
						<para>Mode based on standard deviation ("sd" mode) is applicable for symmetrical distributions. This mode is not recommended for skewed data (see function <link linkend='ST_skewness'>ST_skewness</link>). </para>
						<para>For for skewed - and of course symmetrical -  data both IQR-Modes (iqr15, iqr30) are recommended.</para>
					</listitem>
				</varlistentry>
			</variablelist>
		</para>
	</refsection>
	<refsection>
		<title>Dean-Dixon (ST_deandixon)</title>
		<para>
		A test for outliers of normally distributed data which is particularly simple to apply has been developed by J.W. Dixon. This test is eminently suitable for small sample sizes (less than 30 samples); for samples having more than 30 observations the test for significance of Pearson and Hartley can be used as well. 
	</para>
		<para>
 It sorts the distribution in ascending or descending order, then takes the minimum and maximum values (xi) and calculates the respective Q value for both xi values. This is compared with the critical value from a table (Qcrit). If one of the two  or both Q values greater than the corresponding Qcrit value, one or both xi values are outliers.
    </para>
		<para>
			<latex>
<![CDATA[
\begin{eqnarray}
Q = \left | x_{i+1}-x_i \right |/\left | x_n-x_i \right | \quad ; \quad Q &gt; Q_{crit} \quad \Rightarrow \quad x_i = \text{outlier}
\end{eqnarray}
]]>
	    </latex>
		</para>
		<para>
 	   Only one outlier can be found on each side of the sorted distribution.
    </para>
	</refsection>
	<refsection>
		<title>Significance of Extreme Values (ST_pearsonhartley)</title>
		<para>
		For random samples larger than 30 objects possible outliers may be identified by using the significance thresholds of Pearson and Hartley. 
	</para>
		<para>
 xi is regarded to be an outlier if the test statistic q exceeds the critical threshold qcrit for a given level of significance alpha (statistical confidence level) and a sample size n. 
    </para>
		<para>
			<latex>
<![CDATA[
\begin{eqnarray}
q=\left| \frac{1}{s}(x_i - \bar{x}) \right| \quad ; \quad q &gt;q_{crit} \quad  \Rightarrow \quad x_i = \text{outlier} \\
x_i: \text{test value} \quad ; \quad \bar{x}: \text{arithmetic mean} \quad ; \quad s: \text{standard deviation}
\end{eqnarray}
]]>
	    </latex>
		</para>
	</refsection>
	<refsection>
		<title>Nalimov (ST_nalimov)</title>
		<para>
		Nalimov is applicable for data sets between 3 and 1000 values. It calculates for all values the test value "q". It compares these q-values with the appropriate qcrit value from a table (Kaiser/Gottschalk 1972).
	</para>
		<para>
			<latex>
<![CDATA[
\begin{eqnarray}
q = \left | \frac{1}{s}(x_i- \bar{x}) \right | \sqrt{\frac{n}{n-1}} \quad;\quad q&gt;q_{crit}\;\Rightarrow \; x_i=\text{outlier} \\
x_i: \text{test value} \quad ; \quad \bar{x}: \text{arithmetic mean} \\
s: \text{standard deviation of samples} \quad ; \quad n: \text{number of values}
\end{eqnarray}
]]>
	    </latex>
		</para>
		<para>
		The Nalimov test widely known in Germany and German-speaking countries. But it has it flaws. It indicates outliers very strict. That could eliminates valid data in some circumstances. There is some scientific discussion about this test. It is thus recommended to use other tests instead of the Nalimov test (Dean-Dixon test for small samples, the test of Pearson and Hartley for larger ones).
    </para>
		<important>
			<para>
		If you want to use Nalimov, use it with care.
	</para>
		</important>
	</refsection>
	<refsection>
		<title>ST_outlier</title>
		<para>
		Provide two basic tests on outliers based on standard deviation (S.D.) or inter-quartile range (IQR). It should be applied to sample sizes larger than 10 better more than 25. For small sample sizes the Dean-Dixon test is recommended.
	</para>
		<variablelist>
			<varlistentry>
				<term>Based on Standard deviation:</term>
				<listitem>
					<para>
			 If we assume a normal distribution, a single value may be considered as an outlier if it falls outside a certain range of the standard deviation. In many cases a factor of 2.5 is used, which means that approx. 99 % of the data belonging to a normal distribution fall inside this range. You have to be sure that your data is not skewed. You can test this with ST_skewed() in SampleSTAT.
    </para>
					<para>
						<latex>
<![CDATA[
\begin{eqnarray}
(\bar{x} - 2.5\sigma) &gt; x_i &gt; (\bar{x} + 2.5\sigma) \; \text{with} \quad \sigma = \sqrt{{1 \over n}\sum_{i=1}^{n}(x_i-\bar{x})^2} \quad \Rightarrow \quad x_i = outlier\\
x_i: \text{value} \quad ; \quad \bar{x}: \text{arithmetic mean} \\
\sigma: \text{standard deviation of population} \quad ; \quad n: \text{number of values}
\end{eqnarray}
]]>
    </latex>
					</para>
					<para>
		Sample STAT provides the mode "sd" for testing on outliers regarding on S.D.
	</para>
				</listitem>
			</varlistentry>
			<varlistentry>
				<term>Based on he interquartile range (IQR)</term>
				<listitem>
					<para>
			 The above-mentioned strategies for identifying outliers are probably most appropriate for symmetric unimodal distributions. If a distribution is skewed, it is recommended to calculate the threshold for outliers from the interquartile distance. </para>
			 <para>
				 x0.25 is the first quartile and x0.75 the third and IQR is the difference. 
    </para>
					<para>
						<latex><![CDATA[
\begin{eqnarray}
IQR = x_{0.75} - x_{0.25} \\
(x_{0.25} - 1.5 \cdot IQR)  < x_i < (x_{0.25} + 1.5 \cdot IQR)  \quad \Rightarrow \quad x_i = \text{weak outlier (iqr15 mode)} \\
(x_{0.25} - 3.0 \cdot IQR)  < x_i < (x_{0.25} + 3.0 \cdot IQR)  \quad \Rightarrow \quad x_i = \text{strong outlier (iqr30 mode)}
\end{eqnarray}
]]></latex>
					</para>
					<para>
		Values which are weak but not strong outliers are also called extreme values. SampleSTAT provides the modes "iqr15" (1.5xIQR) and "iqr30" (3.0xIQR) for testing on weak or strong outliers. 
	</para>
					<para>
		Please note that these basic tests require at least 10 observations (better 25, or more). 
	</para>
				</listitem>
			</varlistentry>
		</variablelist>
	</refsection>
	<refsection>
		<title>Authors</title>
		<para>
		Hani A. Ibrahim - hani.ibrahim@gmx.de
	</para>
	</refsection>
	<refsection>
		<title>Credits</title>
		<para>Most of the text and images are from Lohringer, H., "Fundamentals of Statistics", Oct, 10th, 2012, <ulink url="http://www.statistics4u.info/fundstat_eng/">http://www.statistics4u.info/fundstat_eng/</ulink></para>
	</refsection>
	<refsection>
		<title>Bibliography</title>
		<para>Lohringer, H., "Fundamentals of Statistics", Oct, 10th, 2012, <ulink url="http://www.statistics4u.info/fundstat_eng/">http://www.statistics4u.info/fundstat_eng/</ulink></para>
		<para>R. Kaiser, G. Gottschalk; "Elementare Tests zur Beurteilung von Meßdaten", BI Hochschultaschenbücher, Bd. 774, Mannheim 1972.</para>
	</refsection>
</refentry>
